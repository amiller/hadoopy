

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Welcome to Hadoopy’s documentation! &mdash; Hadoopy v.01 documentation</title>
    <link rel="stylesheet" href="_static/default.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '',
        VERSION:     '.01',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <link rel="top" title="Hadoopy v.01 documentation" href="#" /> 
  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li><a href="#">Hadoopy v.01 documentation</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="welcome-to-hadoopy-s-documentation">
<h1>Welcome to Hadoopy&#8217;s documentation!<a class="headerlink" href="#welcome-to-hadoopy-s-documentation" title="Permalink to this headline">¶</a></h1>
<div class="toctree-wrapper compound">
<ul class="simple">
</ul>
</div>
<div class="section" id="example-hello-wordcount">
<h2>Example - Hello Wordcount!<a class="headerlink" href="#example-hello-wordcount" title="Permalink to this headline">¶</a></h2>
<p>Python Source (fully documented version in tests/wc.py)</p>
<div class="highlight-python"><div class="highlight"><pre><span class="sd">&quot;&quot;&quot;Hadoopy Wordcount Demo&quot;&quot;&quot;</span>
<span class="kn">import</span> <span class="nn">hadoopy</span>

<span class="k">def</span> <span class="nf">mapper</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">value</span><span class="o">.</span><span class="n">split</span><span class="p">():</span>
        <span class="k">yield</span> <span class="n">word</span><span class="p">,</span> <span class="mi">1</span>

<span class="k">def</span> <span class="nf">reducer</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">values</span><span class="p">):</span>
    <span class="n">accum</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">count</span> <span class="ow">in</span> <span class="n">values</span><span class="p">:</span>
        <span class="n">accum</span> <span class="o">+=</span> <span class="nb">int</span><span class="p">(</span><span class="n">count</span><span class="p">)</span>
    <span class="k">yield</span> <span class="n">key</span><span class="p">,</span> <span class="n">accum</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">hadoopy</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">mapper</span><span class="p">,</span> <span class="n">reducer</span><span class="p">,</span> <span class="n">doc</span><span class="o">=</span><span class="n">__doc__</span><span class="p">)</span>
</pre></div>
</div>
<p>Command line test (run without args prints docstring and quits because of doc=__doc__)</p>
<div class="highlight-python"><pre>$ python wc.py
Hadoopy Wordcount Demo</pre>
</div>
<p>Command line test (map)</p>
<div class="highlight-python"><pre>$ echo "a b a a b c" | python wc.py map
a    1
b    1
a    1
a    1
b    1
c    1</pre>
</div>
<p>Command line test (map/sort)</p>
<div class="highlight-python"><pre>$ echo "a b a a b c" | python wc.py map | sort
a    1
a    1
a    1
b    1
b    1
c    1</pre>
</div>
<p>Command line test (map/sort/reduce)</p>
<div class="highlight-python"><pre>$ echo "a b a a b c" | python wc.py map | sort | python wc.py reduce
a    3
b    2
c    1</pre>
</div>
<p>Here are a few test files</p>
<div class="highlight-python"><pre>$ hadoop fs -ls playground/
Found 3 items
-rw-r--r--   2 brandyn supergroup     259835 2011-01-17 18:56 /user/brandyn/playground/wc-input-alice.tb
-rw-r--r--   2 brandyn supergroup     167529 2011-01-17 18:56 /user/brandyn/playground/wc-input-alice.txt
-rw-r--r--   2 brandyn supergroup      60638 2011-01-17 18:56 /user/brandyn/playground/wc-input-alice.txt.gz</pre>
</div>
<p>We can also do this in Python</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">hadoopy</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hadoopy</span><span class="o">.</span><span class="n">ls</span><span class="p">(</span><span class="s">&#39;playground/&#39;</span><span class="p">)</span>
<span class="go">[&#39;/user/brandyn/playground/wc-input-alice.tb&#39;, &#39;/user/brandyn/playground/wc-input-alice.txt&#39;, &#39;/user/brandyn/playground/wc-input-alice.txt.gz&#39;]</span>
</pre></div>
</div>
<p>Lets put wc-input-alice.txt through the word counter using Hadoop.  Each node in the cluster has Hadoopy installed (later we will show that it isn&#8217;t necessary with launch_frozen).  Note that it is using typedbytes, SequenceFiles, and the AutoInputFormat by default.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">cmd</span> <span class="o">=</span> <span class="n">hadoopy</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="s">&#39;playground/wc-input-alice.txt&#39;</span><span class="p">,</span> <span class="s">&#39;playground/out/&#39;</span><span class="p">,</span> <span class="s">&#39;wc.py&#39;</span><span class="p">)</span>
<span class="go">HadooPY: Running[hadoop jar /usr/lib/hadoop-0.20/contrib/streaming/hadoop-streaming-0.20.2+737.jar -output playground/out/ -input playground/wc-input-alice.txt -mapper &quot;python wc.py map&quot; -reducer &quot;python wc.py reduce&quot; -file wc.py -jobconf mapred.job.name=python wc.py -io typedbytes -outputformat org.apache.hadoop.mapred.SequenceFileOutputFormat -    inputformat AutoInputFormat]</span>
<span class="go">11/01/17 20:22:31 WARN streaming.StreamJob: -jobconf option is deprecated, please use -D instead.</span>
<span class="go">packageJobJar: [wc.py, /var/lib/hadoop-0.20/cache/brandyn/hadoop-unjar464849802654976085/] [] /tmp/streamjob1822202887260165136.jar tmpDir=null</span>
<span class="go">11/01/17 20:22:32 INFO mapred.FileInputFormat: Total input paths to process : 1</span>
<span class="go">11/01/17 20:22:32 INFO streaming.StreamJob: getLocalDirs(): [/var/lib/hadoop-0.20/cache/brandyn/mapred/local]</span>
<span class="go">11/01/17 20:22:32 INFO streaming.StreamJob: Running job: job_201101141644_0723</span>
<span class="go">11/01/17 20:22:32 INFO streaming.StreamJob: To kill this job, run:</span>
<span class="go">11/01/17 20:22:32 INFO streaming.StreamJob: /usr/lib/hadoop-0.20/bin/hadoop job  -Dmapred.job.tracker=umiacs.umd.edu:8021 -kill job_201101141644_0723</span>
<span class="go">11/01/17 20:22:32 INFO streaming.StreamJob: Tracking URL: http://umiacs.umd.edu:50030/jobdetails.jsp?jobid=job_201101141644_0723</span>
<span class="go">11/01/17 20:22:33 INFO streaming.StreamJob:  map 0%  reduce 0%</span>
<span class="go">11/01/17 20:22:40 INFO streaming.StreamJob:  map 50%  reduce 0%</span>
<span class="go">11/01/17 20:22:41 INFO streaming.StreamJob:  map 100%  reduce 0%</span>
<span class="go">11/01/17 20:22:52 INFO streaming.StreamJob:  map 100%  reduce 100%</span>
<span class="go">11/01/17 20:22:55 INFO streaming.StreamJob: Job complete: job_201101141644_0723</span>
<span class="go">11/01/17 20:22:55 INFO streaming.StreamJob: Output: playground/out/</span>
</pre></div>
</div>
<p>Return value is the command used</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="k">print</span><span class="p">(</span><span class="n">cmd</span><span class="p">)</span>
<span class="go">hadoop jar /usr/lib/hadoop-0.20/contrib/streaming/hadoop-streaming-0.20.2+737.jar -output playground/out/ -input playground/wc-input-alice.txt -mapper &quot;python wc.py map&quot; -reducer &quot;python wc.py reduce&quot; -file wc.py -jobconf mapred.job.name=python wc.py -io typedbytes -outputformat org.apache.hadoop.mapred.SequenceFileOutputFormat -inputformat AutoInputFormat</span>
</pre></div>
</div>
<p>Lets see what the output looks like.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">hadoopy</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="s">&#39;playground/out&#39;</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span>
<span class="go">[(&#39;*&#39;, 60), (&#39;-&#39;, 7), (&#39;3&#39;, 2), (&#39;4&#39;, 1), (&#39;A&#39;, 8), (&#39;I&#39;, 260), (&#39;O&#39;, 1), (&#39;a&#39;, 662), (&#39;&quot;I&#39;, 7), (&quot;&#39;A&quot;, 9)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="nb">cmp</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span><span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="p">:]</span>
<span class="go">[(&#39;was&#39;, 329), (&#39;it&#39;, 356), (&#39;in&#39;, 401), (&#39;said&#39;, 416), (&#39;she&#39;, 484), (&#39;of&#39;, 596), (&#39;a&#39;, 662), (&#39;to&#39;, 773), (&#39;and&#39;, 780), (&#39;the&#39;, 1664)]</span>
</pre></div>
</div>
<p>Note that the output is stored in SequenceFiles and each key/value is stored encoded as TypedBytes, by using cat you don&#8217;t have to worry about any of that (if the output was compressed it would also be decompressed).  This can also be done inside of a job for getting additional side-data.</p>
<p>What if we don&#8217;t want to install python, numpy, scipy, or your-custom-code-that-always-changes on every node in the cluster?  We have you covered there too.  I&#8217;ll remove hadoopy from all nodes except for the one executing the job.</p>
<div class="highlight-python"><pre>$ sudo rm -r /usr/local/lib/python2.6/dist-packages/hadoopy*</pre>
</div>
<p>Now it&#8217;s gone</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">hadoopy</span>
<span class="gt">Traceback (most recent call last):</span>
  File <span class="nb">&quot;&lt;stdin&gt;&quot;</span>, line <span class="m">1</span>, in <span class="n">&lt;module&gt;</span>
<span class="gr">ImportError</span>: <span class="n">No module named hadoopy</span>
</pre></div>
</div>
<p>The rest of the nodes were cleaned up the same way.  We modify the command, note that we now get the output from cx_Freeze at the top</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">cmd</span> <span class="o">=</span> <span class="n">hadoopy</span><span class="o">.</span><span class="n">launch_frozen</span><span class="p">(</span><span class="s">&#39;playground/wc-input-alice.txt&#39;</span><span class="p">,</span> <span class="s">&#39;playground/out_frozen/&#39;</span><span class="p">,</span> <span class="s">&#39;wc.py&#39;</span><span class="p">)</span>
<span class="go">Missing modules:</span>
<span class="go">? _md5 imported from hashlib</span>
<span class="go">? _scproxy imported from urllib</span>
<span class="go">? _sha imported from hashlib</span>
<span class="go">? _sha256 imported from hashlib</span>
<span class="go">? _sha512 imported from hashlib</span>

<span class="go">HadooPY: Running[hadoop jar /usr/lib/hadoop-0.20/contrib/streaming/hadoop-streaming-0.20.2+737.jar -output playground/out_frozen/ -input playground/wc-input-alice.txt -mapper &quot;wc map&quot; -reducer &quot;wc reduce&quot; -file frozen/_codecs_tw.so -file frozen/_codecs_cn.so -file frozen/sgmlop.so -file frozen/_codecs_iso2022.so -file frozen/_main.so -file frozen/_ssl.so -file frozen/_codecs_hk.so -file frozen/_codecs_jp.so -file frozen/_multibytecodec.so -file frozen/datetime.so -file frozen/_codecs_kr.so -file frozen/mmap.so -file frozen/readline.so -file frozen/_heapq.so -file frozen/bz2.so -file frozen/_typedbytes.so -file frozen/_ctypes.so -file frozen/_hashlib.so -file frozen/_multiprocessing.so -file frozen/pyexpat.so -file frozen/libpython2.6.so.1.0 -file frozen/termios.so -file frozen/wc -jobconf mapred.job.name=wc -io typedbytes -outputformat org.apache.hadoop.mapred.SequenceFileOutputFormat -inputformat AutoInputFormat]</span>
<span class="go">11/01/17 20:55:00 WARN streaming.StreamJob: -jobconf option is deprecated, please use -D instead.</span>
<span class="go">packageJobJar: [frozen/_codecs_tw.so, frozen/_codecs_cn.so, frozen/sgmlop.so, frozen/_codecs_iso2022.so, frozen/_main.so, frozen/_ssl.so, frozen/_codecs_hk.so, frozen/_codecs_jp.so, frozen/_multibytecodec.so, frozen/datetime.so, frozen/_codecs_kr.so, frozen/mmap.so, frozen/readline.so, frozen/_heapq.so, frozen/bz2.so, frozen/_typedbytes.so, frozen/_ctypes.so, frozen/_hashlib.so, frozen/_multiprocessing.so, frozen/pyexpat.so, frozen/libpython2.6.so.1.0, frozen/termios.so, frozen/wc, /var/lib/hadoop-0.20/cache/brandyn/hadoop-unjar6437825264052222661/] [] /tmp/streamjob9089438158340520087.jar tmpDir=null</span>
<span class="go">11/01/17 20:55:02 INFO mapred.FileInputFormat: Total input paths to process : 1</span>
<span class="go">11/01/17 20:55:02 INFO streaming.StreamJob: getLocalDirs(): [/var/lib/hadoop-0.20/cache/brandyn/mapred/local]</span>
<span class="go">11/01/17 20:55:02 INFO streaming.StreamJob: Running job: job_201101141644_0724</span>
<span class="go">11/01/17 20:55:02 INFO streaming.StreamJob: To kill this job, run:</span>
<span class="go">11/01/17 20:55:02 INFO streaming.StreamJob: /usr/lib/hadoop-0.20/bin/hadoop job  -Dmapred.job.tracker=umiacs.umd.edu:8021 -kill job_201101141644_0724</span>
<span class="go">11/01/17 20:55:02 INFO streaming.StreamJob: Tracking URL: http://umiacs.umd.edu:50030/jobdetails.jsp?jobid=job_201101141644_0724</span>
<span class="go">11/01/17 20:55:03 INFO streaming.StreamJob:  map 0%  reduce 0%</span>
<span class="go">11/01/17 20:55:09 INFO streaming.StreamJob:  map 50%  reduce 0%</span>
<span class="go">11/01/17 20:55:11 INFO streaming.StreamJob:  map 100%  reduce 0%</span>
<span class="go">11/01/17 20:55:21 INFO streaming.StreamJob:  map 100%  reduce 100%</span>
<span class="go">11/01/17 20:55:24 INFO streaming.StreamJob: Job complete: job_201101141644_0724</span>
<span class="go">11/01/17 20:55:24 INFO streaming.StreamJob: Output: playground/out_frozen/</span>
</pre></div>
</div>
<p>And lets check the output</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">hadoopy</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="s">&#39;playground/out_frozen&#39;</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span>
<span class="go">[(&#39;*&#39;, 60), (&#39;-&#39;, 7), (&#39;3&#39;, 2), (&#39;4&#39;, 1), (&#39;A&#39;, 8), (&#39;I&#39;, 260), (&#39;O&#39;, 1), (&#39;a&#39;, 662), (&#39;&quot;I&#39;, 7), (&quot;&#39;A&quot;, 9)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="nb">cmp</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span><span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="p">:]</span>
<span class="go">[(&#39;was&#39;, 329), (&#39;it&#39;, 356), (&#39;in&#39;, 401), (&#39;said&#39;, 416), (&#39;she&#39;, 484), (&#39;of&#39;, 596), (&#39;a&#39;, 662), (&#39;to&#39;, 773), (&#39;and&#39;, 780), (&#39;the&#39;, 1664)]</span>
</pre></div>
</div>
<p>We can also generate a tar of the frozen script (useful when working with Oozie).  Note the &#8216;wc&#8217; is not wc.py, it is actually a self contained executable.</p>
<div class="highlight-python"><pre>$ python wc.py freeze wc.tar.gz
Missing modules:
? _md5 imported from hashlib
? _scproxy imported from urllib
? _sha imported from hashlib
? _sha256 imported from hashlib
? _sha512 imported from hashlib
$ tar -tzf wc.tar.gz
_codecs_tw.so
_codecs_cn.so
sgmlop.so
_codecs_iso2022.so
_main.so
_codecs_hk.so
_codecs_jp.so
_multibytecodec.so
datetime.so
_codecs_kr.so
mmap.so
readline.so
_heapq.so
bz2.so
_typedbytes.so
_ctypes.so
_multiprocessing.so
pyexpat.so
libpython2.6.so.1.0
termios.so
wc</pre>
</div>
<p>Lets open it up and try it out</p>
<div class="highlight-python"><pre>$ tar -xzf wc.py
$ ./wc
Hadoopy Wordcount Demo
$ python wc.py
Hadoopy Wordcount Demo
$ hexdump -C wc | head -n5
00000000  7f 45 4c 46 02 01 01 00  00 00 00 00 00 00 00 00  |.ELF............|
00000010  02 00 3e 00 01 00 00 00  80 41 40 00 00 00 00 00  |..&gt;......A@.....|
00000020  40 00 00 00 00 00 00 00  50 04 16 00 00 00 00 00  |@.......P.......|
00000030  00 00 00 00 40 00 38 00  09 00 40 00 1d 00 1c 00  |....@.8...@.....|
00000040  06 00 00 00 05 00 00 00  40 00 00 00 00 00 00 00  |........@.......|</pre>
</div>
<p>That&#8217;s a quick tour of Hadoopy.</p>
</div>
<div class="section" id="api">
<h2>API<a class="headerlink" href="#api" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="hadoopy.run">
<tt class="descclassname">hadoopy.</tt><tt class="descname">run</tt><big>(</big><em>mapper=None</em>, <em>reducer=None</em>, <em>combiner=None</em>, <em>**kw</em><big>)</big><a class="headerlink" href="#hadoopy.run" title="Permalink to this definition">¶</a></dt>
<dd><p>This is to be called in all Hadoopy job&#8217;s.  Handles arguments passed in, calls the provided functions with input, and stores the output.</p>
<p>TypedBytes are used if (os.environ[&#8216;stream_map_input&#8217;] == &#8216;typedbytes&#8217;) which is a jobconf variable set in all jobs Hadoop is using TypedBytes with.</p>
<p>It is <em>highly</em> recommended that TypedBytes be used for all non-trivial tasks.  Keep in mind that the semantics of what you can safely emit from your functions is limited when using Text (i.e., no \t or \n).  You can use the base64 module to ensure that your output is clean.</p>
<div class="line-block">
<div class="line"><strong>Command Interface</strong></div>
<div class="line">The command line switches added to your script (e.g., script.py) are</div>
</div>
<dl class="docutils">
<dt>python script.py map</dt>
<dd>Use the provided mapper</dd>
<dt>python script.py reduce</dt>
<dd>Use the provided reducer</dd>
<dt>python script.py combine</dt>
<dd>Use the provided combiner</dd>
<dt>python script.py freeze &lt;tar_path&gt; &lt;-Zadd_file0 -Zadd_file1...&gt; &lt;cx_Freeze args&gt;</dt>
<dd>Freeze the script to a tar file specified by &lt;tar_path&gt;.  The extension may be .tar or .tar.gz.  All files are placed in the root of the tar. Files specified with -Z will be added to the tar root.  Additional cx_Freeze arguments may be passed in.</dd>
</dl>
<div class="line-block">
<div class="line"><strong>Specification of mapper/reducer/combiner</strong></div>
<div class="line">Input Key/Value Types</div>
</div>
<blockquote>
<div><div class="line-block">
<div class="line">For TypedBytes, the type will be the decoded typed</div>
<div class="line">For Text, the type will be text assuming key0\tvalue0\nkey1\tvalue1\n</div>
</div>
</div></blockquote>
<p>Output Key/Value Types</p>
<blockquote>
<div><div class="line-block">
<div class="line">For TypedBytes, anything Pickle-able can be used</div>
<div class="line">For Text, types are converted to string.  Note that neither may contain \t or \n as these are used in the encoding.  Output is key\tvalue\n</div>
</div>
</div></blockquote>
<p>Expected arguments</p>
<blockquote>
<div><div class="line-block">
<div class="line">mapper(key, value) or mapper.map(key, value)</div>
<div class="line">reducer(key, values) or reducer.reduce(key, values)</div>
<div class="line">combiner(key, values) or combiner.combine(key, values)</div>
</div>
</div></blockquote>
<p>Optional methods</p>
<blockquote>
<div><div class="line-block">
<div class="line">func.configure(): Call first.  Returns None.</div>
<div class="line">func.close():  Call last.  Returns Iterator of (key, value) or None</div>
</div>
</div></blockquote>
<dl class="docutils">
<dt>Expected return</dt>
<dd>Iterator of (key, value) or None</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>mapper</strong> &#8211; Function or class instance following the above spec</li>
<li><strong>reducer</strong> &#8211; Function or class instance following the above spec</li>
<li><strong>combiner</strong> &#8211; Function or class instance following the above spec</li>
<li><strong>doc</strong> &#8211; If specified, on error print this and call sys.exit(1)</li>
</ul>
</td>
</tr>
<tr class="field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">True on error, else False (may not return if doc is set and
there is an error)</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="hadoopy.status">
<tt class="descclassname">hadoopy.</tt><tt class="descname">status</tt><big>(</big><em>msg</em>, <em>err=None</em><big>)</big><a class="headerlink" href="#hadoopy.status" title="Permalink to this definition">¶</a></dt>
<dd><p>Output a status message that is displayed in the Hadoop web interface</p>
<p>The status message will replace any other, if you want to append you must
do this yourself.</p>
<dl class="docutils">
<dt>Args:</dt>
<dd>msg: String representing the status message
err: Func that outputs a string if None stderr is used (default None)</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="hadoopy.counter">
<tt class="descclassname">hadoopy.</tt><tt class="descname">counter</tt><big>(</big><em>group</em>, <em>counter</em>, <em>amount=1</em>, <em>err=None</em><big>)</big><a class="headerlink" href="#hadoopy.counter" title="Permalink to this definition">¶</a></dt>
<dd><p>Output a counter update that is displayed in the Hadoop web interface</p>
<p>Counters are useful for quickly identifying the number of times an error
occurred, current progress, or coarse statistics.</p>
<dl class="docutils">
<dt>Args:</dt>
<dd>group: Counter group
counter: Counter name
amount: Value to add (default 1)
err: Func that outputs a string if None stderr is used (default None)</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="hadoopy.launch">
<tt class="descclassname">hadoopy.</tt><tt class="descname">launch</tt><big>(</big><em>in_name</em>, <em>out_name</em>, <em>script_path</em>, <em>mapper=True</em>, <em>reducer=True</em>, <em>combiner=False</em>, <em>partitioner=False</em>, <em>files=()</em>, <em>jobconfs=()</em>, <em>cmdenvs=()</em>, <em>copy_script=True</em>, <em>hstreaming=None</em>, <em>name=None</em>, <em>use_typedbytes=True</em>, <em>use_seqoutput=True</em>, <em>use_autoinput=True</em>, <em>pretend=False</em>, <em>add_python=True</em>, <em>**kw</em><big>)</big><a class="headerlink" href="#hadoopy.launch" title="Permalink to this definition">¶</a></dt>
<dd><p>Run Hadoop given the parameters</p>
<dl class="docutils">
<dt>Args:</dt>
<dd><p class="first">in_name: Input path (string or list)
out_name: Output path
script_path: Path to the script (e.g., script.py)
mapper: If True, the mapper is &#8220;script.py map&#8221;.</p>
<blockquote>
<div>If string, the mapper is the value</div></blockquote>
<dl class="docutils">
<dt>reducer: If True (default), the reducer is &#8220;script.py reduce&#8221;.</dt>
<dd>If string, the reducer is the value</dd>
<dt>combiner: If True, the reducer is &#8220;script.py combine&#8221; (default False).</dt>
<dd>If string, the combiner is the value</dd>
</dl>
<p>partitioner: If True, the partitioner is the value.
copy_script: If True, the script is added to the files list.
files: Extra files (other than the script) (string or list).</p>
<blockquote>
<div>NOTE: Hadoop copies the files into working directory (path errors!).</div></blockquote>
<p class="last">jobconfs: Extra jobconf parameters (string or list)
cmdenvs: Extra cmdenv parameters (string or list)
hstreaming: The full hadoop streaming path to call.
use_typedbytes: If True (default), use typedbytes IO.
use_seqoutput: True (default), output sequence file. If False, output is text.
use_autoinput: If True (default), sets the input format to auto.
pretend: If true, only build the command and return.
add_python: If true, use &#8216;python script_name.py&#8217;</p>
</dd>
<dt>Returns:</dt>
<dd>The hadoop command called.</dd>
<dt>Raises:</dt>
<dd>subprocess.CalledProcessError: Hadoop error.
OSError: Hadoop streaming not found.</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="hadoopy.launch_frozen">
<tt class="descclassname">hadoopy.</tt><tt class="descname">launch_frozen</tt><big>(</big><em>in_name</em>, <em>out_name</em>, <em>script_path</em>, <em>**kw</em><big>)</big><a class="headerlink" href="#hadoopy.launch_frozen" title="Permalink to this definition">¶</a></dt>
<dd><p>Freezes a script and then launches it.</p>
<p>Consult hadoopy._freeze.freeze and hadoopy.launch for optional kw args.</p>
<dl class="docutils">
<dt>Args:</dt>
<dd>in_name: Input path (string or list)
out_name: Output path
script_path: Path to the script (e.g., script.py)</dd>
<dt>Returns:</dt>
<dd>A tuple of the freeze and hadoop commands.</dd>
<dt>Raises:</dt>
<dd>subprocess.CalledProcessError: Hadoop or Cxfreeze error.
OSError: Hadoop streaming or Cxfreeze not found.</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="hadoopy.cat">
<tt class="descclassname">hadoopy.</tt><tt class="descname">cat</tt><big>(</big><em>path</em>, <em>procs=10</em><big>)</big><a class="headerlink" href="#hadoopy.cat" title="Permalink to this definition">¶</a></dt>
<dd><p>Read typedbytes sequence files on HDFS (with optional compression).</p>
<dl class="docutils">
<dt>Args:</dt>
<dd>path: A string (potentially with wildcards).
procs: Number of processes to use.</dd>
<dt>Returns:</dt>
<dd>An iterator of key, value pairs.</dd>
<dt>Raises:</dt>
<dd>IOError: An error occurred listing the directory (e.g., not available).</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="hadoopy.ls">
<tt class="descclassname">hadoopy.</tt><tt class="descname">ls</tt><big>(</big><em>path</em><big>)</big><a class="headerlink" href="#hadoopy.ls" title="Permalink to this definition">¶</a></dt>
<dd><p>List files on HDFS.</p>
<dl class="docutils">
<dt>Args:</dt>
<dd>path: A string (potentially with wildcards).</dd>
<dt>Returns:</dt>
<dd>A list of strings representing HDFS paths.</dd>
<dt>Raises:</dt>
<dd>IOError: An error occurred listing the directory (e.g., not available).</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="hadoopy.GroupedValues">
<em class="property">class </em><tt class="descclassname">hadoopy.</tt><tt class="descname">GroupedValues</tt><a class="headerlink" href="#hadoopy.GroupedValues" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="class">
<dt id="hadoopy.Test">
<em class="property">class </em><tt class="descclassname">hadoopy.</tt><tt class="descname">Test</tt><big>(</big><em>*args</em>, <em>**kw</em><big>)</big><a class="headerlink" href="#hadoopy.Test" title="Permalink to this definition">¶</a></dt>
<dd><dl class="method">
<dt id="hadoopy.Test.groupby_kv">
<tt class="descname">groupby_kv</tt><big>(</big><em>kv</em><big>)</big><a class="headerlink" href="#hadoopy.Test.groupby_kv" title="Permalink to this definition">¶</a></dt>
<dd><p>Group sorted KeyValue pairs</p>
<dl class="docutils">
<dt>Args:</dt>
<dd>kv: Iterator of KeyValue pairs</dd>
<dt>Returns:</dt>
<dd>Grouped KeyValue pairs in sorted order</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="hadoopy.Test.shuffle_kv">
<tt class="descname">shuffle_kv</tt><big>(</big><em>kv</em><big>)</big><a class="headerlink" href="#hadoopy.Test.shuffle_kv" title="Permalink to this definition">¶</a></dt>
<dd><p>Given KeyValue pairs, sort, then group</p>
<dl class="docutils">
<dt>Args:</dt>
<dd>kv: Iterator of KeyValue pairs</dd>
<dt>Returns:</dt>
<dd>Grouped KeyValue pairs in sorted order</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="hadoopy.Test.sort_kv">
<tt class="descname">sort_kv</tt><big>(</big><em>kv</em><big>)</big><a class="headerlink" href="#hadoopy.Test.sort_kv" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform a stable sort on KeyValue pair keys</p>
<dl class="docutils">
<dt>Args:</dt>
<dd>kv: Iterator of KeyValue pairs</dd>
<dt>Returns:</dt>
<dd>Grouped KeyValue pairs in sorted order</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="hadoopy.TypedBytesFile">
<em class="property">class </em><tt class="descclassname">hadoopy.</tt><tt class="descname">TypedBytesFile</tt><a class="headerlink" href="#hadoopy.TypedBytesFile" title="Permalink to this definition">¶</a></dt>
<dd><dl class="attribute">
<dt id="hadoopy.TypedBytesFile.next">
<tt class="descname">next</tt><a class="headerlink" href="#hadoopy.TypedBytesFile.next" title="Permalink to this definition">¶</a></dt>
<dd><p>x.next() -&gt; the next value, or raise StopIteration</p>
</dd></dl>

</dd></dl>

</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
  <h3><a href="#">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Welcome to Hadoopy&#8217;s documentation!</a><ul>
<li><a class="reference internal" href="#example-hello-wordcount">Example - Hello Wordcount!</a></li>
<li><a class="reference internal" href="#api">API</a></li>
</ul>
</li>
</ul>

  <h3>This Page</h3>
  <ul class="this-page-menu">
    <li><a href="_sources/index.txt"
           rel="nofollow">Show Source</a></li>
  </ul>
<div id="searchbox" style="display: none">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" size="18" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li><a href="#">Hadoopy v.01 documentation</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright 2011, Brandyn White.
      Created using <a href="http://sphinx.pocoo.org/">Sphinx</a> 1.0.7.
    </div>
  </body>
</html>